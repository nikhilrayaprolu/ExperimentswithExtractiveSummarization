#!/usr/bin/env python
#coding:utf8
from __future__ import print_function,division
import models
import argparse,random,logging,numpy,os
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.autograd import Variable
from torch.utils.data import DataLoader
from torch.nn.utils import clip_grad_norm
from time import time
from tqdm import tqdm

logging.basicConfig(level=logging.INFO, format='%(asctime)s [INFO] %(message)s')
parser = argparse.ArgumentParser(description='extractive summary')
# model
parser.add_argument('-save_dir',type=str,default='checkpoints/')
parser.add_argument('-embed_dim',type=int,default=100)
parser.add_argument('-embed_num',type=int,default=100)
parser.add_argument('-pos_dim',type=int,default=50)
parser.add_argument('-pos_num',type=int,default=100)
parser.add_argument('-seg_num',type=int,default=10)
parser.add_argument('-model',type=str,default='BasicRNN')
parser.add_argument('-hidden_size',type=int,default=100)
# train
parser.add_argument('-lr',type=float,default=1e-3)
parser.add_argument('-batch_size',type=int,default=32)
parser.add_argument('-epochs',type=int,default=5)
parser.add_argument('-seed',type=int,default=1)
parser.add_argument('-train_dir',type=str,default='data/train.pt')
parser.add_argument('-val_dir',type=str,default='data/val.pt')
parser.add_argument('-vocab',type=str,default='data/vocab.pt')
parser.add_argument('-report_every',type=int,default=1500)
parser.add_argument('-seq_trunc',type=int,default=50)
parser.add_argument('-max_norm',type=float,default=1.0)
# test
parser.add_argument('-load_dir',type=str,default='checkpoints/RNN_seed_1.pt')
parser.add_argument('-test_dir',type=str,default='data/test.pt')
parser.add_argument('-ref',type=str,default='outputs/ref')
parser.add_argument('-hyp',type=str,default='outputs/hyp')
parser.add_argument('-topk',type=int,default=3)
# device
parser.add_argument('-device',type=int,default=0)
# option
parser.add_argument('-test',action='store_true')
parser.add_argument('-debug',action='store_true')
parser.add_argument('-predict',action='store_true')
args = parser.parse_args()

# set cuda device and seed
torch.cuda.set_device(args.device)
torch.cuda.manual_seed(args.seed)
torch.manual_seed(args.seed)
random.seed(args.seed)
numpy.random.seed(args.seed)

def eval(net,vocab,data_iter,criterion):
    net.eval()
    total_loss = 0
    batch_num = 0
    for batch in data_iter:
        features,targets,_,doc_lens = vocab.make_features(batch)
        features,targets = Variable(features).cuda(),Variable(targets.float()).cuda()
        probs = net(features,doc_lens)
        loss = criterion(probs,targets)
        total_loss += loss.data[0]
        batch_num += 1
    loss = total_loss / batch_num
    net.train()
    return loss

def train():
    logging.info('Loading vocab,train and val dataset.Wait a second,please')

    vocab = torch.load(args.vocab)
    embed = vocab.embed
    # update args
    args.embed_num = embed.size(0)
    #print(args.embed_num)
    args.embed_dim = embed.size(1)
    # build model
    net = getattr(models,args.model)(args,embed).cuda()
    # load dataset
    train_iter = DataLoader(dataset=torch.load(args.train_dir),
            batch_size=args.batch_size,
            shuffle=True)
    val_iter = DataLoader(dataset=torch.load(args.val_dir),
            batch_size=args.batch_size,
            shuffle=False)
    # loss function
    #print(len(train_iter))
    criterion = nn.BCELoss()
    # model info
    print(net)
    params = sum(p.numel() for p in list(net.parameters())) / 1e6
    print('#Params: %.1fM' % (params))

    min_loss = float('inf')
    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, net.parameters()),lr=args.lr)
    net.train()

    t1 = time()
    for epoch in range(1,args.epochs+1):
        print(epoch)
        for i,batch in enumerate(train_iter):
            print(i)
            #print(i)
            #print(batch)
            features,targets,_,doc_lens = vocab.make_features(batch)
            #print(features.shape, targets.shape)
            features,targets = Variable(features).cuda(),Variable(targets.float()).cuda()
            #print(i)
            probs = net(features,doc_lens)
            #print("came here")
            loss = criterion(probs,targets)
            optimizer.zero_grad()
            #print("before loas backward")
            loss.backward()
            #print("after loss backward")
            clip_grad_norm(net.parameters(), args.max_norm)
            #print("before optimiser")
            optimizer.step()
            #print("after optimiser")
            if args.debug:
                if i%100 == 0:
                    print('Batch ID:%d Loss:%f' %(i,loss.data[0]))
                continue
            if i % args.report_every == 0:
                cur_loss = eval(net,vocab,val_iter,criterion)
                if cur_loss < min_loss:
                    min_loss = cur_loss
                    best_path = net.save(args)
                logging.info('Epoch: %2d Min_Val_Loss: %f Cur_Val_Loss: %f'
                        % (epoch,min_loss,cur_loss))
    t2 = time()
    logging.info('Total Cost:%f h'%((t2-t1)/3600))

def test():

    vocab = torch.load(args.vocab)
    test_dataset = torch.load(args.test_dir)
    test_iter = DataLoader(dataset=test_dataset,
                            batch_size=args.batch_size,
                            shuffle=False)
    checkpoint = torch.load(args.load_dir)
    net = getattr(models,args.model)(checkpoint['args'])
    net.load_state_dict(checkpoint['model'])
    net.cuda()
    net.eval()

    doc_num = len(test_dataset)
    time_cost = 0
    file_id = 1
    for batch in tqdm(test_iter):
        features,_,summaries,doc_lens = vocab.make_features(batch)
        t1 = time()
        probs = net(Variable(features).cuda(),doc_lens)
        t2 = time()
        time_cost += t2 - t1
        start = 0
        for doc_id,doc_len in enumerate(doc_lens):
            stop = start + doc_len
            prob = probs[start:stop]
            topk = min(3,doc_len)
            topk_indices = prob.topk(topk)[1].cpu().data.numpy()
            topk_indices.sort()
            doc = batch['doc'][doc_id].split('\n')[:doc_len]
            hyp = [doc[index] for index in topk_indices]
            ref = summaries[doc_id]
            with open(os.path.join(args.ref,str(file_id)+'.txt'),'w') as f:
                f.write(ref)
            with open(os.path.join(args.hyp,str(file_id)+'.txt'),'w') as f:
                f.write('\n'.join(hyp))
            start = stop
            file_id = file_id + 1
    print('Speed: %.2f docs / s' % (doc_num / time_cost))
def predict():
    # TODO
    pass
if __name__=='__main__':
    if args.test:
        test()
    else:
        train()
